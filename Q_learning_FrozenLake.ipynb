{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wockeshuh/CSCI166/blob/main/Q_learning_FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frozen Lake Domain Description\n",
        "\n",
        "Frozen Lake is a simple grid-world environment where an agent navigates a frozen lake to reach a goal while avoiding falling into holes. The environment is represented as a grid, with each cell being one of the following:\n",
        "\n",
        "* **S**: Starting position of the agent\n",
        "* **F**: Frozen surface, safe to walk on\n",
        "* **H**: Hole, falling into one ends the episode with a reward of 0\n",
        "* **G**: Goal, reaching it ends the episode with a reward of 1\n",
        "\n",
        "The agent can take four actions:\n",
        "\n",
        "* **0: Left**\n",
        "* **1: Down**\n",
        "* **2: Right**\n",
        "* **3: Up**\n",
        "\n",
        "However, due to the slippery nature of the ice, the agent might not always move in the intended direction. There's a chance it moves perpendicular to the intended direction.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hzTUHNC0Oien"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nKf_jjk9OgN1",
        "outputId": "430cdfb8-0538-486f-fe66-d55385c17979",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "  (Right)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "for _ in range(5):\n",
        "    action = env.action_space.sample()  # Choose a random action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # Render the environment to see the agent's movement (text-based)\n",
        "    if done:\n",
        "        observation= env.reset()\n",
        "    else:\n",
        "      rendered = env.render()\n",
        "      if len(rendered) > 1:  # Check if there's a second element\n",
        "         print(rendered[1])  # Print the second element\n",
        "# Close the environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transition model for the Frozen Lake world describes how the agent's actions affect its movement and the resulting state transitions. Here's a breakdown of the key components:\n",
        "\n",
        "**Actions:**\n",
        "\n",
        "* The agent can choose from four actions:\n",
        "    * 0: Left\n",
        "    * 1: Down\n",
        "    * 2: Right\n",
        "    * 3: Up\n",
        "\n",
        "**State Transitions:**\n",
        "\n",
        "* **Intended Movement:** Ideally, the agent moves one cell in the chosen direction.\n",
        "* **Slippery Ice:** Due to the slippery nature of the ice, there's a probability that the agent will move in a perpendicular direction instead of the intended one. The exact probabilities depend on the specific Frozen Lake environment configuration, but typically:\n",
        "    * **Successful Move:** The agent moves in the intended direction with a high probability.\n",
        "    * **Perpendicular Move:** The agent moves 90 degrees to the left or right of the intended direction with a lower probability.\n",
        "* **Boundaries:** If the intended movement would take the agent outside the grid boundaries, it remains in its current position.\n",
        "* **Holes:** If the agent lands on a hole (\"H\"), the episode ends, and it receives a reward of 0.\n",
        "* **Goal:** If the agent reaches the goal (\"G\"), the episode ends, and it receives a reward of 1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R_q5-OvYOldL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "for _ in range(5):\n",
        "    action = env.action_space.sample()  # Choose a random action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # Render the environment to see the agent's movement (text-based)\n",
        "    if done:\n",
        "        observation= env.reset()\n",
        "    else:\n",
        "      rendered = env.render()\n",
        "      if len(rendered) > 1:  # Check if there's a second element\n",
        "         print(rendered[1])  # Print the second element\n",
        "# Close the environment\n",
        "env.close()\n",
        "print (\"State 14 Going Right: (s, a, r, Done)\", env.P[14][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7nU_-9uaOQR",
        "outputId": "b21dd632-8b81-49eb-ae68-13a6c85caace"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "State 14 Going Right: (s, a, r, Done) [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create FrozenLake environment\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "# Starter code for students (modified for number of iterations)\n",
        "def value_iteration(env, gamma=0.9, num_iterations=1000):\n",
        "    \"\"\"\n",
        "    Implements the Value Iteration algorithm.\n",
        "\n",
        "    Args:\n",
        "        env: The OpenAI Gym environment.\n",
        "        gamma: Discount factor.\n",
        "        num_iterations: Number of iterations to run.\n",
        "\n",
        "    Returns:\n",
        "        The optimal value function and policy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize value function and policy\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    # TODO: Implement the core Value Iteration logic here\n",
        "    # - Iterate for 'num_iterations'\n",
        "    # - For each state:\n",
        "    #   - Calculate Q values for all actions\n",
        "    #   - Update V[s] with the max Q value\n",
        "    #   - Update policy[s] with the action that maximizes Q value\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        for s in range(env.observation_space.n):\n",
        "            q_values = np.zeros(env.action_space.n)\n",
        "            for a in range(env.action_space.n):\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    q_values[a] += prob * (reward + gamma * V[next_state])\n",
        "            V[s] = np.max(q_values)\n",
        "            policy[s] = np.argmax(q_values)\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "\n",
        "# Apply student's Value Iteration\n",
        "optimal_V, optimal_policy = value_iteration(env)\n",
        "\n",
        "# Evaluate student's solution (Optional)\n",
        "def evaluate_policy(env, policy, num_episodes=100):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "    return total_reward / num_episodes\n",
        "\n",
        "average_reward = evaluate_policy(env, optimal_policy)\n",
        "print(\"Optimal Value:\")\n",
        "print(optimal_V.reshape(4,4))\n",
        "print(\"Optimal Policy:\")\n",
        "print(optimal_policy.reshape(4,4))\n",
        "print(\"Average Reward:\", average_reward)"
      ],
      "metadata": {
        "id": "U92a-f0HO1j7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "695b9a2c-df5a-4098-d3a9-bf4f76c92be3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value:\n",
            "[[0.0688909  0.06141457 0.07440976 0.05580732]\n",
            " [0.09185454 0.         0.11220821 0.        ]\n",
            " [0.14543635 0.24749695 0.29961759 0.        ]\n",
            " [0.         0.3799359  0.63902015 0.        ]]\n",
            "Optimal Policy:\n",
            "[[0. 3. 0. 3.]\n",
            " [0. 0. 0. 0.]\n",
            " [3. 1. 0. 0.]\n",
            " [0. 2. 1. 0.]]\n",
            "Average Reward: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning Algorithm"
      ],
      "metadata": {
        "id": "Smji7_nz9CXV"
      }
    },
    {
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "def q_learning(env, gamma=0.9, alpha=0.8, epsilon=0.1, num_episodes=10000):\n",
        "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Epsilon-Greedy action selection\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Q-Learning update\n",
        "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "            state = next_state\n",
        "\n",
        "    # Extract optimal policy from Q-value function\n",
        "    policy = np.argmax(Q, axis=1)\n",
        "\n",
        "     return Q, policy\n",
        "\n",
        "   Q, q_policy = q_learning(env)\n",
        "\n",
        "   print(\"Optimal Q-value:\")\n",
        "   print(Q)\n",
        "   print(\"Optimal policy:\")\n",
        "   print(q_policy)\n",
        "\n",
        "def evaluate_policy(env, policy, num_episodes=100):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "    return total_reward / num_episodes\n",
        "\n",
        "  q_average_reward = evaluate_policy(env, q_policy)\n",
        "  print(\"Average reward:\", q_average_reward)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z44_f_ehKtI3",
        "outputId": "e13320b5-aa1f-4389-d571-9a1d93b39280"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Q-value:\n",
            "[[1.98254195e-01 4.71453074e-02 3.90545823e-02 6.41041617e-02]\n",
            " [7.26987785e-06 1.50121367e-02 4.34277177e-04 7.24105753e-02]\n",
            " [6.18277696e-02 1.01308981e-02 9.59322092e-03 9.08654910e-03]\n",
            " [2.33829200e-03 2.18387962e-03 7.31026210e-03 9.24939417e-03]\n",
            " [2.79336169e-01 3.85900444e-03 1.74680319e-02 1.78611829e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.75024213e-02 1.31538791e-07 4.12187912e-03 4.39108588e-07]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.39744736e-02 2.90466245e-02 3.84246691e-02 3.84886345e-01]\n",
            " [3.56508735e-02 2.50165649e-01 1.03099289e-01 2.25410923e-02]\n",
            " [7.64802333e-01 2.34471063e-04 3.33046453e-03 8.39743655e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.61882553e-02 1.50945349e-01 8.13639985e-01 5.06277433e-02]\n",
            " [2.40836731e-01 7.81935085e-01 9.83952618e-01 7.10466656e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Optimal policy:\n",
            "[0 3 0 3 0 0 0 0 3 1 0 0 0 2 2 0]\n",
            "Average reward: 0.62\n"
          ]
        }
      ]
    },
    {
      "source": [
        "print(\"Q-Learning vs Value Iteration policy difference:\")\n",
        "print((q_policy != optimal_policy).reshape(4, 4))\n",
        "print(\"Q-Learning vs Value Iteration value difference:\")\n",
        "print((np.max(Q, axis=1) - optimal_V).reshape(4, 4))\n",
        "print(\"Q-Learning Average reward:\", q_average_reward)\n",
        "print(\"Value Iteration Average reward:\", average_reward)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-c1eq8yMGnT",
        "outputId": "6093744a-f9e0-469b-d3a6-cf05dcd1dc70"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison:\n",
            "Q-Learning vs Value Iteration policy difference:\n",
            "[[False False False False]\n",
            " [False False False False]\n",
            " [False False False False]\n",
            " [False False  True False]]\n",
            "Q-Learning vs Value Iteration value difference:\n",
            "[[ 0.12936329  0.010996   -0.01258199 -0.04655793]\n",
            " [ 0.18748163  0.         -0.07470579  0.        ]\n",
            " [ 0.23944999  0.00266869  0.46518474  0.        ]\n",
            " [ 0.          0.43370408  0.34493247  0.        ]]\n",
            "Q-Learning Average reward: 0.62\n",
            "Value Iteration Average reward: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The policies for Q-learning algorithm and Value Iteration were the same for every state that has a False. There is only one state that is True and this represents where the two algorithm's policies diverge. From this, I think we can assume that the algorithms have similar approaches to solving a problem.\n",
        "\n",
        "The difference in values show which states the algorithms favored. The positive values represent a state that Q-learning favored and negative values represent the states that Value Iteration favored. The Q-learning algorithm valued states that were more towards the left. This can be because it was trying to learn new actions. The Value Iteration algorithm favored the right side more. This can be because the goal is at the bottom right so it's trying to find the most optimal way there.\n",
        "\n",
        "The average reward shows that Value Iteration is more efficient. I think this is because the Q-learning algorithm tries to solve a problem through trial and error.\n"
      ],
      "metadata": {
        "id": "iXmL8wvnZRiN"
      }
    }
  ]
}